'''
Runs the streamlit app
Call this file in the terminal via `streamlit run app.py`
'''
import streamlit as st

from streamlit_extras.colored_header import colored_header
from streamlit_option_menu import option_menu
from streamlit_extras.badges import badge
from streamlit_card import card

# from utils import *
from utils1 import check_nan, download_button, load_model

from prettytable import PrettyTable

import sqlite3
import os

import requests
from openai import OpenAI

import pandas as pd

import joblib

# import os
# import streamlit as st
# from streamlit_option_menu import option_menu
# from streamlit_extras.colored_header import colored_header
# from prettytable import PrettyTable




#         page_title="MLMD",
#         page_icon="ğŸ",
#         layout="centered",
#         initial_sidebar_state="auto",
#         menu_items={
#         })

# sysmenu = '''
# <style>
# MainMenu {visibility:hidden;}
# footer {visibility:hidden;}
# '''

# # https://icons.bootcss.com/
# st.markdown(sysmenu,unsafe_allow_html=True)
DEFAULT_STORAGE_PATH = "data"


def HEA_PPID():
    with st.sidebar:
        select_option = option_menu("", ["Home Page", "Model Inference", "Chat with Model"],

                                    menu_icon="boxes", default_index=0)
        st.write('''
            **Contact**: 



    ''')
        # æ­¤å‡½æ•°ä¸ºå®šä¹‰ä¸»é¡µé¢çš„é€‰æ‹©æ¡†åç»­ä¸ºé€‰æ‹©æ¡†çš„å„ä¸ªå†…å®¹ã€‚

    if select_option == "Home Page":
        colored_header(label="Model Inference", description=" ", color_name="violet-90")

        # é¡¹ç›®ç®€ä»‹
        st.subheader("é¡¹ç›®ç®€ä»‹")
        st.write("""
        æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé«˜ç†µåˆé‡‘ç½‘ç»œé¢„æµ‹å¹³å°ï¼Œæ—¨åœ¨ä¸ºæ— äººå·¥æ™ºèƒ½åŸºç¡€çš„ææ–™ç ”ç©¶è€…æä¾›ä¾¿æ·çš„å·¥å…·å’Œæ¨¡å‹ã€‚è¯¥å¹³å°åŸºäºæœ€æ–°çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹é«˜ç†µåˆé‡‘çš„æ€§èƒ½ï¼Œæ¨åŠ¨åˆé‡‘ææ–™çš„ç ”å‘å’Œåº”ç”¨ã€‚
        """)

        # ä½¿ç”¨çš„ç®—æ³•
        st.subheader("ä½¿ç”¨çš„ç®—æ³•")
        st.write("""
        - **æ”¯æŒå‘é‡æœº (SVM)**
        - **TabPFNå›å½’**
        - **XGBoost**
        """)

        # é¡¹ç›®æˆå‘˜
        st.subheader("é¡¹ç›®æˆå‘˜")
        st.write("""
        - **é¡¹ç›®ç»„é•¿ï¼š** å‘¨å¤©æ¯…
        - **ç»„å‘˜ï¼š** ï¼ˆå…·ä½“æˆå‘˜å¾…å®šï¼‰
        - **æŒ‡å¯¼è€å¸ˆï¼š** ç†Šæ°
        """)

        # è”ç³»æ–¹å¼
        st.subheader("è”ç³»æ–¹å¼")
        st.write("è¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»é¡¹ç›®å›¢é˜Ÿï¼š")
        st.write("Email: example@example.com")

    elif select_option == "Model Inference":


        # ===============================

        # 1ï¸âƒ£ è‡ªåŠ¨åŠ è½½æ¨¡å‹

        # ===============================

        model_folder = r'G:\HEA_PPID\model_done'

        # æ‰¾åˆ°æ‰€æœ‰ .pkl æ¨¡å‹ï¼ˆæ’é™¤ scaler/pcaï¼‰

        model_files = [f for f in os.listdir(model_folder)

                       if f.endswith('.pkl') and 'scaler' not in f and 'pca' not in f]

        if not model_files:

            st.error("No model file found in model_done!")

            model = None

        else:

            # ä¸‹æ‹‰é€‰æ‹©æ¨¡å‹

            selected_model_name = st.selectbox("Select Model", model_files)

            try:

                model = joblib.load(os.path.join(model_folder, selected_model_name))

                st.success(f"Loaded model: {selected_model_name}")

            except Exception as e:

                st.error(f"Failed to load model: {e}")

                model = None

        # ===============================

        # 2ï¸âƒ£ å±•ç¤ºæ ‡é¢˜

        # ===============================

        colored_header(label="Model Inference", description="Enter metal composition to predict TS", color_name="violet-90")

        # ===============================

        # 3ï¸âƒ£ ç”¨æˆ·è¾“å…¥é‡‘å±æ¯”ä¾‹ï¼ˆ22 ä¸ªå…ƒç´ ï¼‰

        # ===============================

        FEATURES = [
            'Co', 'Cr', 'Fe', 'Ni', 'Mn', 'Nb', 'Al', 'Ti', 'C',
            'Mo', 'Si', 'Cu', 'V', 'Y', 'Sn', 'Li', 'Mg', 'Zn',
            'Ta', 'Zr', 'Hf', 'W'
        ]

        if 'metal_ratios' not in st.session_state:
            st.session_state['metal_ratios'] = [0.0] * len(FEATURES)

        # åˆ†ä¸¤è¡Œæ˜¾ç¤ºï¼Œæ¯è¡Œ 11 åˆ—
        first_row_features = FEATURES[:11]
        second_row_features = FEATURES[11:]

        # ç¬¬ä¸€è¡Œ
        cols1 = st.columns(11)
        for i, feature in enumerate(first_row_features):
            with cols1[i]:
                st.session_state['metal_ratios'][i] = st.number_input(
                    feature, min_value=0.0, max_value=100.0, step=0.01,
                    key=f"metal_ratio_{i}"
                )

        # ç¬¬äºŒè¡Œ
        cols2 = st.columns(11)
        for j, feature in enumerate(second_row_features):
            with cols2[j]:
                st.session_state['metal_ratios'][j + 11] = st.number_input(
                    feature, min_value=0.0, max_value=100.0, step=0.01,
                    key=f"metal_ratio_{j + 11}"
                )

        # æ€»å’Œæ˜¾ç¤º
        ratio_total = round(sum(st.session_state['metal_ratios']), 2)
        st.markdown(f"**Total Ratio = {ratio_total} %**")

        # ===============================

        # 4ï¸âƒ£ éªŒè¯æŒ‰é’® + é¢„æµ‹

        # ===============================

        if st.button("Validate & Predict TS"):

            if model is None:

                st.error("No model loaded!")

            elif ratio_total != 100.0:

                st.error("Error: Total ratio must equal 100%")

            else:

                # å‡†å¤‡å®Œæ•´ DataFrame

                input_df = pd.DataFrame([st.session_state['metal_ratios']], columns=FEATURES)

                # å±•ç¤ºè¾“å…¥

                st.subheader("Input Composition")

                st.dataframe(input_df)

                try:

                    # ä½¿ç”¨ Pipeline é¢„æµ‹ï¼ˆåŒ…å« scaler + PCA + SVRï¼‰

                    predicted_ts = model.predict(input_df)[0]

                    st.subheader("Prediction Result")

                    st.write(f"Predicted TS (MPa): {predicted_ts:.2f}")


                except Exception as e:

                    st.error(f"Prediction failed: {e}")

            # # æœ€åä¸€åˆ—ï¼šæ˜¾ç¤ºæ€»æ¯”ä¾‹
            # with cols[-1]:
            #     ratio_total = round(sum(metal_ratios), 2)
            #     st.markdown("**Total Ratio**")
            #     st.write(f"{ratio_total} %")
            #
            # if 'df_ready' in locals():
            #     df = df_ready  # ç”¨æˆ·æ‰‹åŠ¨ç”Ÿæˆçš„è¾“å…¥æ•°æ®
            #
            #     # æ£€æŸ¥ NaN
            #     if df.isnull().any().any():
            #         st.error("Error: Input contains NaN values!")
            #         st.stop()
            #
            #     # æ˜¾ç¤ºè¾“å…¥æ•°æ®
            #     st.subheader("Input Data")
            #     st.write(df)
            #
            #     # ====== æ•°æ®é¢„å¤„ç† ======
            #     # è¿™é‡Œç›´æ¥ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„ scaler å’Œ pca
            #     features_scaled = scaler.transform(df.values)  # StandardScaler
            #     features_pca = pca.transform(features_scaled)  # PCAé™ç»´
            #
            #     # ======== é¢„æµ‹éƒ¨åˆ† ========
            #     st.subheader("Run Prediction")
            #
            #     if model_selected and st.button("Run Prediction"):
            #         try:
            #             # æ¨¡å‹å·²ç»åŠ è½½å¥½äº† joblib.load(model_path)
            #             prediction = model.predict(features_pca)
            #
            #             # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
            #             st.subheader("Prediction Result")
            #             st.write(f"Predicted value: {prediction[0]:.2f}")
            #
            #             # å¯é€‰ï¼šç”Ÿæˆä¸‹è½½ CSV
            #             result_data = df.copy()
            #             result_data['Predicted'] = prediction
            #             csv_file = result_data.to_csv(index=False)
            #             st.download_button("Download Prediction CSV", csv_file, "prediction.csv", "text/csv")
            #
            #         except FileNotFoundError:
            #             st.error(f"Model file not found: {model_path}")


        # elif model_source == '[2] Upload your own model (Please upload your model here)':
        #
        #     file = st.file_uploader("Upload `.csv`file", label_visibility="collapsed", accept_multiple_files=True)
        #
        #     if len(file) < 2:
        #         table = PrettyTable(['file name', 'class', 'description'])
        #         table.add_row(['file_1', 'data set (+test data)', 'data file'])
        #         table.add_row(['file_2', 'model', 'model'])
        #         st.write(table)
        #     elif len(file) == 2:
        #         df = pd.read_csv(file[0])
        #         model_file = file[1]
        #
        #         try:
        #             model = pickle.load(model_file)
        #
        #             if hasattr(model, 'n_features_in_'):
        #                 n_model_features = model.n_features_in_
        #                 st.info(f"â„¹ï¸ This model expects **{n_model_features} features** as input.")
        #             else:
        #                 st.warning("âš ï¸ This model does not have `n_features_in_` attribute.")
        #         except Exception as e:
        #             st.error(f"Error loading model: {e}")
        #             model = None
        #             n_model_features = None
        #
        #         check_string_NaN(df)
        #
        #         colored_header(label="Data information", description=" ", color_name="violet-70")
        #         nrow = st.slider("rows", 1, len(df), 5)
        #         df_nrow = df.head(nrow)
        #         st.write(df_nrow)
        #
        #         colored_header(label="Feature and target", description=" ", color_name="violet-70")
        #
        #         target_num = st.number_input('target number', min_value=1, max_value=10, value=1)
        #
        #         col_feature, col_target = st.columns(2)
        #         # features
        #         features = df.iloc[:, :-target_num]
        #         # targets
        #         targets = df.iloc[:, -target_num:]
        #         with col_feature:
        #             st.write(features.head())
        #         with col_target:
        #             st.write(targets.head())
        #         colored_header(label="target", description=" ", color_name="violet-70")
        #
        #         target_selected_option = st.selectbox('target', list(targets)[::-1])
        #
        #         targets = targets[target_selected_option]
        #         preprocess = st.selectbox('data preprocess', [None, 'StandardScaler', 'MinMaxScaler'])
        #         if preprocess == 'StandardScaler':
        #             features = StandardScaler().fit_transform(features)
        #         elif preprocess == 'MinMaxScaler':
        #             features = MinMaxScaler().fit_transform(features)
        #
        #         model = pickle.load(model_file)
        #         prediction = model.predict(features)
        #         # st.write(std)
        #         plot = customPlot()
        #         plot.pred_vs_actual(targets, prediction)
        #         r2 = r2_score(targets, prediction)
        #         st.write('R2: {}'.format(r2))
        #         result_data = pd.concat([targets, pd.DataFrame(prediction)], axis=1)
        #         result_data.columns = ['actual', 'prediction']
        #         with st.expander('prediction'):
        #             st.write(result_data)
        #             tmp_download_link = download_button(result_data, f'prediction.csv', button_text='download')
        #             st.markdown(tmp_download_link, unsafe_allow_html=True)
        #         st.write('---')

    elif select_option == "Chat with Model":
        client = OpenAI(

            api_key="sk-214f8e7ee2e943e2a220cd0fd40058d3",  # âš ï¸å»ºè®®ç”¨ st.secrets æˆ–ç¯å¢ƒå˜é‡ä»£æ›¿æ˜æ–‡å¯†é’¥

            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"

        )

        st.title("ğŸ’¬ Chat with ç™¾ç‚¼ Qwen æ¨¡å‹")

        # ä¼šè¯çŠ¶æ€ç”¨äºä¿å­˜å¯¹è¯å†å²

        if "conversation" not in st.session_state:
            st.session_state.conversation = []

        # æ˜¾ç¤ºå†å²å¯¹è¯

        for msg in st.session_state.conversation:
            st.write(f"**{msg['role']}**: {msg['text']}")

        user_input = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜:")

        if st.button("å‘é€"):

            if user_input:

                # æ·»åŠ ç”¨æˆ·é—®é¢˜åˆ°å¯¹è¯

                st.session_state.conversation.append({"role": "ç”¨æˆ·", "text": user_input})

                try:

                    completion = client.chat.completions.create(

                        model="qwen-plus",

                        messages=[

                            {"role": "system", "content": "You are a helpful assistant."},

                            *[

                                {"role": "user" if m["role"] == "ç”¨æˆ·" else "assistant", "content": m["text"]}

                                for m in st.session_state.conversation if m["role"] != "ç³»ç»Ÿ"

                            ],

                            {"role": "user", "content": user_input}

                        ]

                    )

                    reply = completion.choices[0].message.content

                except Exception as e:

                    reply = f"è¯·æ±‚å¤±è´¥: {e}"

                # æ·»åŠ æ¨¡å‹å›å¤

                st.session_state.conversation.append({"role": "æ¨¡å‹", "text": reply})

                # åˆ·æ–°æ˜¾ç¤ºå¯¹è¯

                for msg in st.session_state.conversation:
                    st.write(f"**{msg['role']}**: {msg['text']}")

            else:

                st.warning("è¯·è¾“å…¥é—®é¢˜åå†ç‚¹å‡»å‘é€ã€‚")


if __name__ == "__main__":
    HEA_PPID()